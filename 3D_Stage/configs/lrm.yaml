# performance: 75GB VRAM, 2.0 it/s
name: lrm_t3072l12d64x16_c4p504_e_tp64x80v96_96_384c96
tag: "v1-bs8"
exp_root_dir: "outputs"
seed: 0

data_cls: lrm.data.objaverse.ObjaverseDataModule
data:
  root_dir: /lvis_100view_new
  # model number: 45468
  scene_list: /lvis_100view_min_mask_percent_0.002.json
  version: v2
  image_suffix: webp
  background_color: [0.5, 0.5, 0.5]

  num_views_per_scene: 100
  num_views_input: 4
  num_views_output: 4

  relative_pose: false
  train_sup_views: random_instant3d
  # eval_input_views: [0, 1, 3, 4]  

  train_indices: [0, 45376]
  val_indices: [45376, 45440]
  test_indices: [45376, 45440]

  cond_width: 504 # multiply of 14
  cond_height: 504

  # Please do not use rand crop for DMTet setting
  height: 96
  width: 96
  rand_max_height: 384
  rand_max_width: 384

  batch_size: 8
  num_workers: 16

  eval_height: 256
  eval_width: 256
  eval_batch_size: 8

system_cls: lrm.systems.multiview_lrm.MultiviewLRM
system:
  check_train_every_n_steps: 500

  camera_embedder_cls: lrm.models.camera.LinearCameraEmbedder
  camera_embedder:
    in_channels: 16
    out_channels: 768
    conditions:
      - c2w_cond

  # image tokenizer transforms input images to tokens
  image_tokenizer_cls: lrm.models.tokenizers.image.DINOV2SingleImageTokenizer
  image_tokenizer:
    pretrained_model_name_or_path: "./models/base"
    width: ${data.cond_width} # unused
    height: ${data.cond_height} # unused
    freeze_backbone_params: false
    enable_memory_efficient_attention: true
    enable_gradient_checkpointing: true
    # camera modulation to the DINO transformer layers
    modulation: true
    modulation_zero_init: true
    modulation_single_layer: true
    modulation_cond_dim: ${system.camera_embedder.out_channels}

  # tokenizer gives a tokenized representation for the 3D scene
  # triplane tokens in this case
  tokenizer_cls: lrm.models.tokenizers.triplane.TriplaneLearnablePositionalEmbedding
  tokenizer:
    plane_size: 32
    num_channels: 512

  # backbone network is a transformer that takes scene tokens (potentially with conditional image tokens)
  # and outputs scene tokens of the same size
  backbone_cls: lrm.models.transformers.transformer_1d.Transformer1D
  backbone:
    in_channels: ${system.tokenizer.num_channels}
    num_attention_heads: 16
    attention_head_dim: 64
    num_layers: 12
    cross_attention_dim: 768 # hard-code, =DINO feature dim
    # camera modulation to the transformer layers
    # if not needed, set norm_type=layer_norm and do not specify cond_dim_ada_norm_continuous
    norm_type: "layer_norm"
    enable_memory_efficient_attention: true
    gradient_checkpointing: true

  # post processor takes scene tokens and outputs the final scene parameters that will be used for rendering
  # in this case, triplanes are upsampled and the features are condensed
  post_processor_cls: lrm.models.networks.TriplaneUpsampleNetwork
  post_processor:
    in_channels: 512
    out_channels: 80

  material_cls: lrm.models.materials.no_material.NoMaterial

  background_cls: lrm.models.background.solid_color_background.SolidColorBackground
  background:
    color: ${data.background_color}

  loss:
    lambda_mse: 1.0
    lambda_smooth_l1: 0.2
    lambda_lpips: 1.0
    lambda_mask: 0.1

  # optimizer definition
  # you can set different learning rates separately for each group of parameters, but note that if you do this you should specify EVERY trainable parameters
  optimizer:
    name: AdamW
    args:
      lr: 4e-4
      betas: [0.9, 0.95]
      weight_decay: 0.05

  scheduler:
    name: SequentialLR
    interval: step
    schedulers:
      - name: LinearLR
        interval: step
        args:
          start_factor: 1e-6
          end_factor: 1.0
          total_iters: ${mul:${trainer.max_epochs},${idiv:10,${trainer.num_nodes}}}
      - name: CosineAnnealingLR
        interval: step
        args:
          T_max: ${calc_num_train_steps:45376,${data.batch_size},${trainer.max_epochs},${trainer.num_nodes}} # 177 * 300
          eta_min: 0.0
    milestones:
      - ${mul:${trainer.max_epochs},${idiv:10,${trainer.num_nodes}}}

trainer:
  num_nodes: 1
  max_epochs: 100
  log_every_n_steps: 1
  num_sanity_val_steps: 1
  check_val_every_n_epoch: 1
  enable_progress_bar: true
  precision: bf16-mixed
  gradient_clip_val: 1.0

checkpoint:
  save_last: true # whether to save at each validation time
  save_top_k: -1
  every_n_epochs: 20 # do not save at all for debug purpose