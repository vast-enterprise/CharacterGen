extends: configs/lrm.yaml

name: refinement-dmtet
tag: "refine"

data_cls: lrm.data.anime_dataset.ObjaverseDataModule
data:
  # model number: 693063 + 192250 = 885313
  root_dir:
    - /new_render

  train_random_background: random_gray
  num_views_per_scene: 20
  image_suffix: png

  height: 448
  width: 448
  rand_max_height: 448
  rand_max_width: 448  

  train_indices: [0, 884801]
  val_indices:
    - 884801
    - ${add:884801,${mul:64,${trainer.num_nodes}}}
  test_indices:
    - 884801
    - ${add:884801,${mul:64,${trainer.num_nodes}}}

system:
  weights: ./models/lrm.ckpt
  # weights_mapping:
  #   - from: decoder.heads.density
  #     to: decoder.heads.sdf
  weights_ignore_modules:
    - decoder.heads.density
  check_train_every_n_steps: 100

  renderer_cls: lrm.models.renderers.triplane_dmtet.TriplaneDMTetRenderer
  renderer:
    radius: 0.6 # slightly larger than 0.5
    feature_reduction: concat
    sdf_bias: -2.
    isosurface_resolution: 256
    enable_isosurface_grid_deformation: false
    sdf_activation: negative  
  
  decoder_cls: lrm.models.networks.MultiHeadMLP
  decoder:
    in_channels: 240 # 3 * 80
    n_neurons: 64
    n_hidden_layers_share: 8
    heads:
      - name: sdf
        out_channels: 1
        n_hidden_layers: 1
        output_activation: null
      - name: features
        out_channels: 3
        n_hidden_layers: 1
        output_activation: null # activate in material
    activation: silu
    chunk_mode: deferred
    chunk_size: 131072      

  # decoder:
  #   chunk_mode: deferred
  #   chunk_size: 131072   

  optimizer:
    name: AdamW
    args:
      lr: 1e-4
      betas: [0.9, 0.95]
      weight_decay: 0.05

  exporter:
    fmt: "obj-mtl"
    #visual: "vertex"
    save_uv: True
    save_texture: True
    uv_unwrap_method: "open3d"
  
  scheduler:
    name: SequentialLR
    interval: step
    schedulers:
      - name: LinearLR
        interval: step
        args:
          start_factor: 1e-6
          end_factor: 1.0
          total_iters: 3000
      - name: CosineAnnealingLR
        interval: step
        args:
          T_max: ${calc_num_train_steps:885313,${data.batch_size},${trainer.max_epochs},${trainer.num_nodes}}
          eta_min: 0.0
    milestones:
      - 3000

trainer:
  max_epochs: 40
  strategy: ddp_find_unused_parameters_true

checkpoint:
  every_n_epochs: 10